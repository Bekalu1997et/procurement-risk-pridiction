{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete MLOps Pipeline Demo\n",
    "## End-to-End Supplier Risk Prediction System\n",
    "\n",
    "**Purpose**: Demonstrate the complete data flow through all pipeline components:\n",
    "- Data Pipeline \u2192 Feature Engineering \u2192 Model Training \u2192 Prediction\n",
    "- Auditing \u2192 Explainability \u2192 NLP \u2192 Visualization \u2192 Recommendations\n",
    "\n",
    "**Why This Matters**: Shows how all components work together in production MLOps workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "**What**: Import all pipeline components\n",
    "**Why**: Each module handles a specific part of the ML lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database initialized successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DABC\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 All pipeline components imported successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root to path\n",
    "BASE_DIR = Path.cwd().parent\n",
    "sys.path.insert(0, str(BASE_DIR))\n",
    "\n",
    "# Import all pipeline components\n",
    "from src import data_pipeline      # Data loading & preprocessing\n",
    "from src import model_pipeline     # Model training & prediction\n",
    "from src import explainability     # SHAP + LLM explanations\n",
    "from src import nlp_layer          # NLP features & summarization\n",
    "from src import auditing           # Data quality & logging\n",
    "from src import visualization      # Charts & plots\n",
    "from src import recommendation     # Business recommendations\n",
    "from backend import visualization_engine  # Advanced visualizations\n",
    "from backend import explainability_viz    # SHAP visualizations\n",
    "\n",
    "print(\"\u2713 All pipeline components imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Pipeline - Load & Preprocess\n",
    "**What**: Load raw data and prepare for modeling\n",
    "**Why**: Clean, validated data is foundation of ML success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " STEP 1: Data Pipeline\n",
      "Training data shape: (10500, 26)\n",
      "Weekly data shape: (10500, 27)\n",
      "\n",
      "Columns: ['supplier_id', 'company_name', 'region', 'industry', 'annual_revenue', 'annual_spend', 'avg_payment_delay_days', 'contract_value', 'contract_duration_months', 'past_disputes']...\n",
      "\n",
      " Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print(\" STEP 1: Data Pipeline\")\n",
    "\n",
    "training_df, weekly_df = data_pipeline.load_processed_datasets()\n",
    "\n",
    "print(f\"Training data shape: {training_df.shape}\")\n",
    "print(f\"Weekly data shape: {weekly_df.shape}\")\n",
    "print(f\"\\nColumns: {list(training_df.columns[:10])}...\")\n",
    "print(\"\\n Data loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Auditing Pipeline - Data Quality Checks\n",
    "**What**: Validate data quality and log metrics\n",
    "**Why**: Catch data issues before they break models (garbage in = garbage out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STEP 2: Auditing Pipeline\n",
      "======================================================================\n",
      "\n",
      "Data Quality Report:\n",
      "                metric  value  threshold  passed\n",
      "0       max_null_ratio    0.0       0.15    True\n",
      "1  region_domain_check    0.0       0.00    True\n",
      "\n",
      "\u2713 Data quality validated and logged\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n STEP 2: Auditing Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Run data quality checks\n",
    "quality_report = auditing.log_data_quality(training_df)\n",
    "\n",
    "print(\"\\nData Quality Report:\")\n",
    "print(quality_report.head(10))\n",
    "\n",
    "# Log audit event\n",
    "auditing.persist_audit_log(\n",
    "    event_type=\"pipeline_execution\",\n",
    "    payload={\n",
    "        \"stage\": \"data_quality_check\",\n",
    "        \"rows\": len(training_df),\n",
    "        \"columns\": len(training_df.columns)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n\u2713 Data quality validated and logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering - Prepare Training Data\n",
    "**What**: Split features (X) and target (y)\n",
    "**Why**: Models need clean separation of inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STEP 3: Feature Engineering\n",
      "======================================================================\n",
      "Features (X) shape: (10500, 9)\n",
      "Target (y) shape: (10500,)\n",
      "\n",
      "Feature columns: ['region', 'industry', 'contract_criticality', 'annual_spend', 'credit_score', 'late_ratio', 'dispute_rate', 'avg_delay', 'clause_risk_score']...\n",
      "\n",
      "Target distribution:\n",
      "risk_label\n",
      "medium    3943\n",
      "high      3743\n",
      "low       2814\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\u2713 Features prepared for training\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n STEP 3: Feature Engineering\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "X, y = data_pipeline.prepare_training_data()\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns[:10])}...\")\n",
    "print(f\"\\nTarget distribution:\\n{y.value_counts()}\")\n",
    "print(\"\\n\u2713 Features prepared for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Pipeline - Train Models\n",
    "**What**: Train Random Forest and XGBoost models\n",
    "**Why**: Ensemble models provide robust predictions with feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\ud83e\udd16 STEP 4: Model Training\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train models\n",
    "artifacts = model_pipeline.train_models(X, y)\n",
    "\n",
    "print(\"\\nModel Training Complete:\")\n",
    "for artifact in artifacts:\n",
    "    print(f\"\\n{artifact.model_name}:\")\n",
    "    print(f\"  Macro F1: {artifact.report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"  Accuracy: {artifact.report['accuracy']:.4f}\")\n",
    "    print(f\"  Model saved: models/{artifact.model_name}.joblib\")\n",
    "\n",
    "print(\"\\n\u2713 Models trained and persisted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prediction Pipeline - Single Supplier Prediction\n",
    "**What**: Predict risk for a single supplier\n",
    "**Why**: Real-time predictions for procurement decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n STEP 5: Prediction Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create test supplier\n",
    "test_supplier = {\n",
    "    \"region\": \"North America\",\n",
    "    \"industry\": \"Manufacturing\",\n",
    "    \"contract_criticality\": \"High\",\n",
    "    \"annual_revenue\": 2000000.0,\n",
    "    \"annual_spend\": 75000.0,\n",
    "    \"avg_payment_delay_days\": 10.0,\n",
    "    \"contract_value\": 150000.0,\n",
    "    \"contract_duration_months\": 12,\n",
    "    \"past_disputes\": 2,\n",
    "    \"delivery_score\": 70.0,\n",
    "    \"financial_stability_index\": 60.0,\n",
    "    \"relationship_years\": 3,\n",
    "    \"txn_count\": 80,\n",
    "    \"avg_txn_amount\": 3000.0,\n",
    "    \"avg_delay\": 8.0,\n",
    "    \"late_ratio\": 0.15,\n",
    "    \"dispute_rate\": 0.08,\n",
    "    \"avg_delivery_quality\": 68.0,\n",
    "    \"clause_risk_score\": 45.0,\n",
    "    \"credit_score\": 650\n",
    "}\n",
    "\n",
    "# Make prediction\n",
    "result = model_pipeline.predict_single(\"random_forest\", test_supplier)\n",
    "\n",
    "print(f\"\\nPrediction: {result['prediction'].upper()}\")\n",
    "print(f\"\\nProbabilities:\")\n",
    "for risk_level, prob in result['probabilities'].items():\n",
    "    print(f\"  {risk_level}: {prob*100:.2f}%\")\n",
    "\n",
    "print(\"\\n\u2713 Prediction generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explainability Pipeline - SHAP + LLM Narratives\n",
    "**What**: Generate human-friendly explanations using SHAP and Ollama\n",
    "**Why**: Regulatory compliance and user trust require explainable AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n STEP 6: Explainability Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build explanation\n",
    "explanation = explainability.build_explanation(\n",
    "    risk_level=result['prediction'],\n",
    "    probabilities=result['probabilities'],\n",
    "    shap_values=result['shap_values'],\n",
    "    feature_names=result['feature_names']\n",
    ")\n",
    "\n",
    "print(f\"\\nRisk Level: {explanation.risk_level.upper()}\")\n",
    "print(f\"Confidence: {explanation.confidence}%\")\n",
    "\n",
    "print(f\"\\nTop 5 Contributing Features:\")\n",
    "for feat, val in explanation.top_features[:5]:\n",
    "    impact = \"increases\" if val > 0 else \"decreases\"\n",
    "    print(f\"  {feat}: {val:.4f} ({impact} risk)\")\n",
    "\n",
    "print(f\"\\nBusiness Narrative:\")\n",
    "print(f\"  {explanation.narrative}\")\n",
    "\n",
    "print(\"\\n\u2713 Explanation generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. NLP Pipeline - Feature Summarization\n",
    "**What**: Use NLP to summarize SHAP features\n",
    "**Why**: Translate technical features to business language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n STEP 8: Recommendation Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = recommendation.build_recommendations(\n",
    "    risk_level=explanation.risk_level,\n",
    "    top_features=explanation.top_features\n",
    ")\n",
    "\n",
    "print(\"\\nActionable Recommendations:\")\n",
    "for i, reco in enumerate(recommendations, 1):\n",
    "    print(f\"  {i}. {reco}\")\n",
    "\n",
    "print(\"\\n\u2713 Recommendations generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recommendation Pipeline - Actionable Insights\n",
    "**What**: Generate business recommendations based on risk\n",
    "**Why**: Predictions without actions are useless - provide next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\ud83d\udcca STEP 9: Visualization Pipeline\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create subplots for feature importance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Left: SHAP bar plot\n",
    "top_features = sorted(zip(explanation.feature_names, abs(explanation.shap_values)), key=lambda x: x[1], reverse=True)[:10]\n",
    "features, values = zip(*top_features)\n",
    "axes[0].barh(range(len(features)), values, color='steelblue')\n",
    "axes[0].set_yticks(range(len(features)))\n",
    "axes[0].set_yticklabels([f.replace('numeric__', '').replace('categorical__', '') for f in features])\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].set_xlabel('|SHAP Value|', fontsize=12)\n",
    "axes[0].set_title('Top 10 SHAP Feature Importance', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Right: Feature importance with direction\n",
    "top_features_signed = sorted(zip(explanation.feature_names, explanation.shap_values), key=lambda x: abs(x[1]), reverse=True)[:10]\n",
    "features_s, values_s = zip(*top_features_signed)\n",
    "colors = ['red' if v > 0 else 'green' for v in values_s]\n",
    "axes[1].barh(range(len(features_s)), values_s, color=colors, alpha=0.7)\n",
    "axes[1].set_yticks(range(len(features_s)))\n",
    "axes[1].set_yticklabels([f.replace('numeric__', '').replace('categorical__', '') for f in features_s])\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].set_xlabel('SHAP Value (Red=Increase Risk, Green=Decrease Risk)', fontsize=12)\n",
    "axes[1].set_title('Feature Impact Direction', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Feature importance visualizations displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualization Pipeline - Feature Importance\n",
    "**What**: Create visual explanations of SHAP values\n",
    "**Why**: Visualizations help stakeholders understand model decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\ud83d\udcc8 STEP 10: Advanced Visualization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pairplot - select key numeric features\n",
    "plot_cols = ['credit_score', 'annual_spend', 'late_ratio', 'dispute_rate', 'risk_label']\n",
    "plot_df = training_df[plot_cols].sample(n=min(500, len(training_df)), random_state=42)\n",
    "\n",
    "print(\"\\nGenerating pairplot (this may take a moment)...\")\n",
    "g = sns.pairplot(plot_df, hue='risk_label', palette={'low': 'green', 'medium': 'orange', 'high': 'red'}, \n",
    "                 diag_kind='kde', plot_kws={'alpha': 0.6}, height=2.5)\n",
    "g.fig.suptitle('Feature Relationships by Risk Level', y=1.02, fontsize=16, fontweight='bold')\n",
    "plt.show()\n",
    "\n",
    "# Subplots: Heatmap and Histogram\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Left: Correlation heatmap\n",
    "numeric_cols = training_df.select_dtypes(include=['float64', 'int64']).columns[:10]\n",
    "corr = training_df[numeric_cols].corr()\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=axes[0], \n",
    "            cbar_kws={'label': 'Correlation'})\n",
    "axes[0].set_title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Right: Credit score distribution by risk\n",
    "for risk in ['low', 'medium', 'high']:\n",
    "    data = training_df[training_df['risk_label'] == risk]['credit_score']\n",
    "    axes[1].hist(data, bins=30, alpha=0.6, label=risk.capitalize())\n",
    "axes[1].set_xlabel('Credit Score', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Credit Score Distribution by Risk Level', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2713 Advanced visualizations displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Advanced Visualization - Data Exploration\n",
    "**What**: Create advanced charts for data analysis\n",
    "**Why**: Understand data distributions and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\ud83d\udcdd STEP 11: Audit Trail\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Log complete pipeline execution\n",
    "auditing.persist_audit_log(\n",
    "    event_type=\"complete_pipeline_execution\",\n",
    "    payload={\n",
    "        \"supplier_id\": \"DEMO_001\",\n",
    "        \"prediction\": result['prediction'],\n",
    "        \"confidence\": explanation.confidence,\n",
    "        \"top_features\": [f[0] for f in explanation.top_features[:3]],\n",
    "        \"recommendations_count\": len(recommendations),\n",
    "        \"visualizations_generated\": 4\n",
    "    }\n",
    ")\n",
    "\n",
    "# Fetch recent audit events with proper column handling\n",
    "try:\n",
    "    recent_events = auditing.db_connector.fetch_audit_trail(limit=5)\n",
    "    print(\"\\nRecent Audit Events:\")\n",
    "    \n",
    "    # Dynamically check which columns exist\n",
    "    available_cols = ['event_type']\n",
    "    if 'created_at' in recent_events.columns:\n",
    "        available_cols.append('created_at')\n",
    "    elif 'timestamp' in recent_events.columns:\n",
    "        available_cols.append('timestamp')\n",
    "    \n",
    "    print(recent_events[available_cols].head())\n",
    "except Exception as e:\n",
    "    print(f\"\\nNote: Could not fetch audit trail from database: {e}\")\n",
    "    print(\"Audit events are logged to CSV at reports/audit_logs/audit_events_log.csv\")\n",
    "\n",
    "print(\"\\n\u2713 Pipeline execution logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Audit Trail - Log Complete Pipeline Execution\n",
    "**What**: Log all pipeline steps for compliance and debugging\n",
    "**Why**: Production systems need complete audit trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\ud83d\udcdd STEP 11: Audit Trail\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Log complete pipeline execution\n",
    "auditing.persist_audit_log(\n",
    "    event_type=\"complete_pipeline_execution\",\n",
    "    payload={\n",
    "        \"supplier_id\": \"DEMO_001\",\n",
    "        \"prediction\": result['prediction'],\n",
    "        \"confidence\": explanation.confidence,\n",
    "        \"top_features\": [f[0] for f in explanation.top_features[:3]],\n",
    "        \"recommendations_count\": len(recommendations),\n",
    "        \"visualizations_generated\": 4\n",
    "    }\n",
    ")\n",
    "\n",
    "# Fetch recent audit events\n",
    "recent_events = auditing.db_connector.fetch_audit_trail(limit=5)\n",
    "\n",
    "print(\"\\nRecent Audit Events:\")\n",
    "print(recent_events[['event_type', 'timestamp']].head())\n",
    "\n",
    "print(\"\\n\u2713 Pipeline execution logged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Complete Pipeline Summary\n",
    "**What**: Summarize entire MLOps workflow\n",
    "**Why**: Show how all components integrate for production ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\ud83c\udf89 COMPLETE MLOPS PIPELINE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary = f\"\"\"\n",
    "\u2713 Data Pipeline: Loaded {len(training_df)} training records\n",
    "\u2713 Auditing: Validated data quality and logged events\n",
    "\u2713 Feature Engineering: Prepared {X.shape[1]} features\n",
    "\u2713 Model Training: Trained Random Forest and XGBoost\n",
    "\u2713 Prediction: Generated {result['prediction'].upper()} risk prediction\n",
    "\u2713 Explainability: Created SHAP + LLM narrative\n",
    "\u2713 NLP: Summarized top {len(top_features_nlp)} features\n",
    "\u2713 Recommendations: Generated {len(recommendations)} action items\n",
    "\u2713 Visualization: Created 4 charts and plots\n",
    "\u2713 Audit Trail: Logged complete pipeline execution\n",
    "\n",
    "Pipeline Components Used:\n",
    "  \u2022 data_pipeline: Data loading & preprocessing\n",
    "  \u2022 model_pipeline: Training & prediction\n",
    "  \u2022 explainability: SHAP + Ollama narratives\n",
    "  \u2022 nlp_layer: Feature summarization\n",
    "  \u2022 auditing: Quality checks & logging\n",
    "  \u2022 visualization: SHAP plots\n",
    "  \u2022 visualization_engine: Advanced charts\n",
    "  \u2022 explainability_viz: Feature importance\n",
    "  \u2022 recommendation: Business actions\n",
    "\n",
    "This demonstrates a complete production MLOps workflow!\n",
    "\"\"\"\n",
    "\n",
    "print(summary)\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}